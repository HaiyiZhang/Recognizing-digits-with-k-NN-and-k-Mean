{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-mean is an iterative clustering analysis algorithm. According to the similarity principle, data objects with high similarity are divided into the same cluster, and data objects with high dissimilarity are divided into different clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main steps of K-mean are:\n",
    "\n",
    "Step1: Suppose we want to cluster N sample observations, requiring clustering into K categories, first select K points as initial center points.\n",
    "\n",
    "Step2: Next, according to the principle of the smallest distance from the initial center point, all observations are divided into the class where each center point is located.\n",
    "\n",
    "Step3: There are several observations in each class, and the mean of all sample points in K classes is calculated as the K center points of the second iteration.\n",
    "\n",
    "Step4: Repeat steps 2 and 3 according to this center until convergence (the center point no longer changes or reaches the specified number of iterations), and the clustering process ends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we will be using the MNIST dataset. The MNIST database is a collection of 60,000 images of handwritten digits from 0 through 9, with numerical labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: train-images-idx3-ubyte.gz\n",
      "Found: train-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "import numpy as np\n",
    "import gzip\n",
    "\n",
    "mnist_url = \"http://yann.lecun.com/exdb/mnist/\"\n",
    "img_file = \"train-images-idx3-ubyte.gz\"\n",
    "labels_file = \"train-labels-idx1-ubyte.gz\"\n",
    "\n",
    "for fname in [img_file, labels_file]:\n",
    "    if Path(fname).is_file() :\n",
    "        print(f\"Found: {fname}\")\n",
    "        continue\n",
    "    print(f\"Downloading: {fname}\")\n",
    "    r = requests.get(mnist_url + fname)\n",
    "    with open(fname, 'wb') as foo:\n",
    "        foo.write(r.content)\n",
    "\n",
    "with gzip.open(img_file, 'rb') as foo:\n",
    "    f = foo.read()\n",
    "images = np.array([b for b in f[16:]]).reshape(-1, 28*28)\n",
    "\n",
    "with gzip.open(labels_file, 'rb') as foo:\n",
    "    f = foo.read()\n",
    "labels = np.array([b for b in f[8:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "images is a two-dimensional array, which contains 60,000 pieces of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels is a one-dimensional array and corresponds to each row of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the size of images and labels, which allows us to make kmean predictions faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images[:5000]\n",
    "labels = labels[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying K-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the k-means algorithm to split MNIST images into 10 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=10)"
      ]
     },
     "execution_count": 659,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km  = KMeans(n_clusters=10)\n",
    "km.fit(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display centroids of the clusters. The centroids of the clusters consist of floating point numbers and the centroids of the clusters do not resemble numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 660,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center = km.cluster_centers_\n",
    "center.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00  0.00000000e+00  0.00000000e+00 -8.88178420e-16  0.00000000e+00\n",
      "  -8.88178420e-16 -8.88178420e-16 -8.88178420e-16  0.00000000e+00 -2.22044605e-16 -3.46944695e-18  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00 -1.38777878e-17\n",
      "  -1.11022302e-16 -4.44089210e-16  0.00000000e+00 -1.77635684e-15 -7.10542736e-15  0.00000000e+00 -7.10542736e-15  2.26000000e+01\n",
      "   3.74000000e+01  1.02000000e+02  1.33000000e+02  1.20600000e+02  1.12800000e+02  4.64000000e+01  8.30000000e+00  0.00000000e+00\n",
      "  -2.77555756e-17  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00 -1.77635684e-15 -3.55271368e-15  2.00000000e-01  9.30000000e+00  2.86000000e+01  3.57000000e+01  1.02400000e+02\n",
      "   1.93200000e+02  2.18400000e+02  2.24300000e+02  1.78200000e+02  1.46000000e+02  9.15000000e+01  1.63000000e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00 -1.11022302e-16 -4.44089210e-16\n",
      "   0.00000000e+00 -3.55271368e-15  0.00000000e+00  1.12000000e+01  3.48000000e+01  6.02000000e+01  1.46400000e+02  2.31400000e+02\n",
      "   2.40900000e+02  2.40700000e+02  2.40700000e+02  2.21800000e+02  1.77500000e+02  1.25200000e+02  4.54000000e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -3.55271368e-15  0.00000000e+00  3.10000000e+00  2.17000000e+01  3.84000000e+01  1.58400000e+02  2.52000000e+02  2.52600000e+02\n",
      "   2.52600000e+02  2.50100000e+02  2.29100000e+02  2.41600000e+02  2.14100000e+02  1.41900000e+02  5.15000000e+01  5.00000000e-01\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00 -1.11022302e-16  0.00000000e+00 -1.77635684e-15\n",
      "   0.00000000e+00  0.00000000e+00  1.62000000e+01  4.37000000e+01  1.53800000e+02  2.41600000e+02  2.52900000e+02  2.52600000e+02\n",
      "   2.49100000e+02  2.06700000e+02  1.88100000e+02  2.33400000e+02  2.27900000e+02  1.69400000e+02  6.02000000e+01  2.40000000e+00\n",
      "  -1.77635684e-15  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  4.00000000e-01  2.66000000e+01  1.36000000e+02  2.47200000e+02  2.52800000e+02  2.53400000e+02  2.40000000e+02\n",
      "   2.02000000e+02  1.61000000e+02  2.07100000e+02  2.29400000e+02  2.14700000e+02  1.54900000e+02  5.01000000e+01  1.17000000e+01\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00 -3.55271368e-15 -7.10542736e-15\n",
      "   4.00000000e-01  1.73000000e+01  1.11700000e+02  2.34600000e+02  2.52600000e+02  2.52400000e+02  2.43500000e+02  1.98600000e+02\n",
      "   1.48600000e+02  1.28500000e+02  1.48000000e+02  2.25100000e+02  2.12800000e+02  1.52200000e+02  5.08000000e+01  1.04000000e+01\n",
      "  -1.77635684e-15  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00 -2.77555756e-17 -1.77635684e-15 -7.10542736e-15  0.00000000e+00\n",
      "   3.20000000e+00  8.73000000e+01  2.04400000e+02  2.48400000e+02  2.52600000e+02  2.34500000e+02  1.93500000e+02  1.24700000e+02\n",
      "   7.37000000e+01  6.06000000e+01  1.42300000e+02  2.29100000e+02  2.25100000e+02  1.17000000e+02  3.43000000e+01  4.00000000e-01\n",
      "  -3.55271368e-15  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00 -2.22044605e-16  0.00000000e+00  0.00000000e+00 -2.84217094e-14\n",
      "   3.68000000e+01  1.85000000e+02  2.33300000e+02  2.49700000e+02  2.39700000e+02  1.99900000e+02  1.47700000e+02  5.56000000e+01\n",
      "   4.71000000e+01  7.21000000e+01  1.57300000e+02  2.35800000e+02  2.11600000e+02  1.01000000e+02  1.56000000e+01 -1.42108547e-14\n",
      "  -3.55271368e-15 -1.11022302e-16 -5.55111512e-17  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  8.70000000e+00\n",
      "   1.33700000e+02  2.26600000e+02  2.49600000e+02  2.46200000e+02  2.04200000e+02  1.45500000e+02  6.30000000e+01  2.02000000e+01\n",
      "   2.48000000e+01  9.81000000e+01  1.95300000e+02  2.51000000e+02  2.04900000e+02  9.65000000e+01  5.90000000e+00  0.00000000e+00\n",
      "   0.00000000e+00 -5.55111512e-17 -6.93889390e-18  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00 -7.10542736e-15 -1.42108547e-14  6.40000000e+01\n",
      "   2.01000000e+02  2.35300000e+02  2.53300000e+02  2.20200000e+02  1.44900000e+02  7.17000000e+01  2.55000000e+01  1.29000000e+01\n",
      "   4.58000000e+01  1.80100000e+02  2.42600000e+02  2.51400000e+02  1.82000000e+02  3.50000000e+01  1.40000000e+00  0.00000000e+00\n",
      "   3.55271368e-15 -1.11022302e-16  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -2.77555756e-17 -1.77635684e-15 -1.42108547e-14  6.50000000e+00  1.17600000e+02\n",
      "   2.16700000e+02  2.45000000e+02  2.47200000e+02  1.75500000e+02  8.89000000e+01  3.75000000e+01  4.40000000e+00  1.42000000e+01\n",
      "   9.70000000e+01  2.13100000e+02  2.42000000e+02  2.33000000e+02  1.13600000e+02  1.43000000e+01  1.42108547e-14  0.00000000e+00\n",
      "   0.00000000e+00  5.55111512e-17  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -6.93889390e-18  0.00000000e+00 -1.42108547e-14  3.58000000e+01  1.65600000e+02\n",
      "   2.40900000e+02  2.52800000e+02  2.19900000e+02  1.33400000e+02  4.70000000e+01  5.30000000e+00  2.70000000e+00  4.94000000e+01\n",
      "   1.88700000e+02  2.51500000e+02  2.34500000e+02  2.07400000e+02  6.03000000e+01  6.00000000e-01  0.00000000e+00 -1.42108547e-14\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.20000000e+00  6.04000000e+01  2.17000000e+02\n",
      "   2.52500000e+02  2.46700000e+02  1.89500000e+02  9.26000000e+01  3.80000000e+01  2.35000000e+01  5.98000000e+01  1.62800000e+02\n",
      "   2.43400000e+02  2.52500000e+02  2.34500000e+02  1.86100000e+02  2.46000000e+01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00  5.30000000e+00  9.87000000e+01  2.50600000e+02\n",
      "   2.52400000e+02  2.37300000e+02  1.68700000e+02  1.05600000e+02  9.13000000e+01  9.05000000e+01  1.87900000e+02  2.40800000e+02\n",
      "   2.52600000e+02  2.42500000e+02  1.91200000e+02  9.43000000e+01  7.80000000e+00 -1.42108547e-14 -7.10542736e-15 -3.55271368e-15\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  3.10000000e+00  9.43000000e+01  2.46400000e+02\n",
      "   2.52800000e+02  2.36800000e+02  1.81600000e+02  1.44900000e+02  1.47000000e+02  2.03800000e+02  2.48100000e+02  2.53100000e+02\n",
      "   2.31200000e+02  2.10700000e+02  1.25500000e+02  2.75000000e+01 -1.42108547e-14  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -4.44089210e-16  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -1.11022302e-16 -3.55271368e-15  3.20000000e+00  7.98000000e+01  2.26000000e+02\n",
      "   2.52500000e+02  2.46000000e+02  2.00400000e+02  1.97800000e+02  2.33900000e+02  2.52500000e+02  2.53000000e+02  2.52700000e+02\n",
      "   2.15900000e+02  1.31700000e+02  4.14000000e+01  1.50000000e+00 -1.42108547e-14 -7.10542736e-15  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00 -2.77555756e-17 -5.55111512e-17  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -1.11022302e-16  0.00000000e+00  1.35000000e+01  5.02000000e+01  1.68600000e+02\n",
      "   2.49000000e+02  2.52900000e+02  2.52600000e+02  2.52500000e+02  2.52900000e+02  2.52500000e+02  2.44100000e+02  2.12500000e+02\n",
      "   1.03900000e+02  3.18000000e+01  2.70000000e+00  0.00000000e+00  0.00000000e+00 -3.55271368e-15  0.00000000e+00 -4.44089210e-16\n",
      "   0.00000000e+00  0.00000000e+00 -1.11022302e-16  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -1.11022302e-16  0.00000000e+00  1.44000000e+01  2.83000000e+01  1.02000000e+02\n",
      "   2.07500000e+02  2.51400000e+02  2.52500000e+02  2.52400000e+02  2.51800000e+02  2.42600000e+02  1.96900000e+02  1.04700000e+02\n",
      "   4.16000000e+01  3.80000000e+00 -7.10542736e-15 -7.10542736e-15 -3.55271368e-15  0.00000000e+00 -4.44089210e-16  0.00000000e+00\n",
      "  -6.93889390e-18 -2.77555756e-17 -5.55111512e-17  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.44000000e+01  2.51000000e+01  3.63000000e+01\n",
      "   1.10700000e+02  1.90500000e+02  2.39500000e+02  2.45000000e+02  1.90800000e+02  1.39400000e+02  5.32000000e+01  8.90000000e+00\n",
      "  -7.10542736e-15  0.00000000e+00  0.00000000e+00 -1.77635684e-15  0.00000000e+00  0.00000000e+00  0.00000000e+00 -2.77555756e-17\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00 -5.55111512e-17  0.00000000e+00 -4.44089210e-16\n",
      "   4.80000000e+00  1.90000000e+01  3.81000000e+01  2.99000000e+01  1.50000000e+01  2.00000000e-01 -3.55271368e-15  0.00000000e+00\n",
      "  -1.77635684e-15  0.00000000e+00 -4.44089210e-16  2.22044605e-16  0.00000000e+00  0.00000000e+00 -1.38777878e-17  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00 -2.77555756e-17  0.00000000e+00  0.00000000e+00  0.00000000e+00 -1.11022302e-16 -5.55111512e-17\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "center = km.cluster_centers_\n",
    "img = center[2].reshape(28, 28)\n",
    "with np.printoptions(linewidth=5*28):\n",
    "    print(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the labels of each input that is generated from K means model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 6, ..., 9, 4, 9], dtype=int32)"
      ]
     },
     "execution_count": 666,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=km.labels_\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But these are not the true labels corresponding to each image. km.labels_ is just the group identity used for the cluster.\n",
    "\n",
    "\n",
    "In order to match it with the corresponding label, I can correspond to the frequency of each label in \"labels\" according to the frequency of each label in \"km.label_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning Cluster Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 644),\n",
       " (8, 640),\n",
       " (3, 569),\n",
       " (7, 563),\n",
       " (1, 506),\n",
       " (6, 497),\n",
       " (0, 435),\n",
       " (2, 398),\n",
       " (9, 375),\n",
       " (5, 373)]"
      ]
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckmlabels=dict()\n",
    "for i in a:\n",
    "    if i in ckmlabels:\n",
    "        ckmlabels[i]+=1\n",
    "    else:\n",
    "        ckmlabels[i]=1\n",
    "sortkm=sorted(ckmlabels.items(), key=lambda d:d[1], reverse = True )  \n",
    "sortkm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 563),\n",
       " (7, 550),\n",
       " (4, 535),\n",
       " (6, 501),\n",
       " (9, 495),\n",
       " (3, 493),\n",
       " (2, 488),\n",
       " (0, 479),\n",
       " (8, 462),\n",
       " (5, 434)]"
      ]
     },
     "execution_count": 676,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cLlabels=dict()\n",
    "for i in labels:\n",
    "    if i in cLlabels:\n",
    "        cLlabels[i]+=1\n",
    "    else:\n",
    "        cLlabels[i]=1\n",
    "sortLl=sorted(cLlabels.items(), key=lambda d:d[1], reverse = True )  \n",
    "sortLl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the frequency, we can infer the labels corresponding to \"km.label_\". For example, 4 in \"km.label_\" corresponds to 1 in \"labels\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {4: 1, 8: 7, 3: 4, 7: 6, 1: 9, 6: 3, 0: 2, 2: 0, 9: 8, 5: 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 3, ..., 8, 1, 8])"
      ]
     },
     "execution_count": 696,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace the elements in \"k_labels\" with the corresponding labels\n",
    "predict =np.zeros(len(a),dtype=int)\n",
    "for i in range(len(a)):\n",
    "    for key in dictionary:\n",
    "        if a[i]==key:\n",
    "            predict[i]=dictionary[key]\n",
    "predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Clustering Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False, ..., False,  True, False])"
      ]
     },
     "execution_count": 693,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how accurately we can predict which image corresponds to which number\n",
    "predict == labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1376"
      ]
     },
     "execution_count": 694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (predictions == labels).sum()/len(labels)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, most of the predicted labels do not match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2a The goal of this part is to experiment with a possible way of making k-NN predictions faster, by reducing the amount of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split MNIST images into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(images,labels,train_size=0.8,test_size=0.2)\n",
    "# the size of X_train is 4000 , X_test is 1000, Y_train is 4000, Y_test is 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each digit 0-9 select all training images corresponding to this digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396 450 388 384 435 339\n"
     ]
    }
   ],
   "source": [
    "a0=[];a1=[];a2=[];a3=[];a4=[];a5=[];a6=[];a7=[];a8=[];a9=[]\n",
    "\n",
    "for i in range(len(Y_train)):\n",
    "    if Y_train[i] == 0:\n",
    "        a0.append(i)\n",
    "    elif Y_train[i] == 1:\n",
    "        a1.append(i)\n",
    "    elif Y_train[i] == 2:\n",
    "        a2.append(i)\n",
    "    elif Y_train[i] == 3:\n",
    "        a3.append(i)\n",
    "    elif Y_train[i] == 4:\n",
    "        a4.append(i)\n",
    "    elif Y_train[i] == 5:\n",
    "        a5.append(i)\n",
    "    elif Y_train[i] == 6:\n",
    "        a6.append(i)\n",
    "    elif Y_train[i] == 7:\n",
    "        a7.append(i)\n",
    "    elif Y_train[i] == 8:\n",
    "        a8.append(i)\n",
    "    elif Y_train[i] == 9:\n",
    "        a9.append(i)\n",
    "print(len(a0),len(a1),len(a2),len(a3),len(a4),len(a5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists0 = [[] for i in range(len(a0))]\n",
    "lists1 = [[] for i in range(len(a1))]\n",
    "lists2 = [[] for i in range(len(a2))]\n",
    "lists3 = [[] for i in range(len(a3))]\n",
    "lists4 = [[] for i in range(len(a4))]\n",
    "lists5 = [[] for i in range(len(a5))]\n",
    "lists6 = [[] for i in range(len(a6))]\n",
    "lists7 = [[] for i in range(len(a7))]\n",
    "lists8 = [[] for i in range(len(a8))]\n",
    "lists9 = [[] for i in range(len(a9))]\n",
    "\n",
    "for i in range(len(a0)):\n",
    "    lists0[i].append(X_train[a0[i]])\n",
    "arr0 = np.array(lists0).reshape(-1,784)\n",
    "\n",
    "for i in range(len(a1)):\n",
    "    lists1[i].append(X_train[a1[i]])\n",
    "arr1 = np.array(lists1).reshape(-1,784)\n",
    "\n",
    "for i in range(len(a2)):\n",
    "    lists2[i].append(X_train[a2[i]])\n",
    "arr2 = np.array(lists2).reshape(-1,784)\n",
    "\n",
    "for i in range(len(a3)):\n",
    "    lists3[i].append(X_train[a3[i]])\n",
    "arr3 = np.array(lists3).reshape(-1,784)\n",
    "\n",
    "for i in range(len(a4)):\n",
    "    lists4[i].append(X_train[a4[i]])\n",
    "arr4 = np.array(lists4).reshape(-1,784)\n",
    "\n",
    "for i in range(len(a5)):\n",
    "    lists5[i].append(X_train[a5[i]])\n",
    "arr5 = np.array(lists5).reshape(-1,784)\n",
    "\n",
    "for i in range(len(a6)):\n",
    "    lists6[i].append(X_train[a6[i]])\n",
    "arr6 = np.array(lists6).reshape(-1,784)\n",
    "\n",
    "for i in range(len(a7)):\n",
    "    lists7[i].append(X_train[a7[i]])\n",
    "arr7 = np.array(lists7).reshape(-1,784)\n",
    "\n",
    "for i in range(len(a8)):\n",
    "    lists8[i].append(X_train[a8[i]])\n",
    "arr8 = np.array(lists8).reshape(-1,784)\n",
    "\n",
    "for i in range(len(a9)):\n",
    "    lists9[i].append(X_train[a9[i]])\n",
    "arr9 = np.array(lists9).reshape(-1,784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST contains images that are 28 by 28 pixels; as a result, they will have a length of 784 once we reshape them into a 1-dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 136 167  26   0  32 233 200  75   8   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  92 247 254 231  48  55 254 254 254  81   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 190 254 254 254 128  31 228 254 254 242  58   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  27 229 254 254 254  21   0  77 233 254 254 188  14   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   2 157 254 254 254 227  15   0   0  33  90 239 254 162   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  57 254 254 254 231  43   0   0   0   0   0  64 250 248  94   1   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  15 188 254 254 252 120   0   0   0   0   0   0   0 177 254 254   5   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  77 254 254 254 206   0   0   0   0   0   0   0   0  44 227 254 126   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 129 254 254 254 135   0   0   0   0   0   0   0   0   0 121 254 252  70   0   0   0   0]\n",
      " [  0   0   0   0   0   0 185 254 254 254  66   0   0   0   0   0   0   0   0   0  66 254 254  99   0   0   0   0]\n",
      " [  0   0   0   0   0   0 185 254 254 190   6   0   0   0   0   0   0   0   0   0  66 254 254 184   0   0   0   0]\n",
      " [  0   0   0   0   0   0 175 254 254 173   0   0   0   0   0   0   0   0   0   3 128 254 254 184   0   0   0   0]\n",
      " [  0   0   0   0   0   0  77 254 254 173   0   0   0   0   0   0   0   0   0 126 254 254 254 112   0   0   0   0]\n",
      " [  0   0   0   0   0   0  47 200 254 239  22   0   0   0   0   0   0   0 122 253 254 254 242  47   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 103 254 254  85   0   0   0   0   0   3  79 253 254 254 243  59   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   5 228 254 135   0   0   0   0   0 125 255 254 254 238  67   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 109 254 230  40   0   0  67 189 252 254 254 254 145   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1 177 254 252 250 250 253 254 254 254 242 152   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  25 206 254 254 254 254 254 254 254  81   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  18  59 183 254 254 254 152  74   7   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "img = arr0[2].reshape(28, 28)\n",
    "with np.printoptions(linewidth=5*28):\n",
    "    print(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use k-means to split these images into 100 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "km0 = KMeans(n_clusters = 100)\n",
    "km0.fit(arr0)\n",
    "k0=km0.cluster_centers_\n",
    "\n",
    "km1 = KMeans(n_clusters = 100)\n",
    "km1.fit(arr1)\n",
    "k1=km1.cluster_centers_\n",
    "\n",
    "km2 = KMeans(n_clusters = 100)\n",
    "km2.fit(arr2)\n",
    "k2=km2.cluster_centers_\n",
    "\n",
    "km3 = KMeans(n_clusters = 100)\n",
    "km3.fit(arr3)\n",
    "k3=km3.cluster_centers_\n",
    "\n",
    "km4 = KMeans(n_clusters = 100)\n",
    "km4.fit(arr4)\n",
    "k4=km4.cluster_centers_\n",
    "\n",
    "km5 = KMeans(n_clusters = 100)\n",
    "km5.fit(arr5)\n",
    "k5=km5.cluster_centers_\n",
    "\n",
    "km6 = KMeans(n_clusters = 100)\n",
    "km6.fit(arr6)\n",
    "k6=km6.cluster_centers_\n",
    "\n",
    "km7 = KMeans(n_clusters = 100)\n",
    "km7.fit(arr7)\n",
    "k7=km7.cluster_centers_\n",
    "\n",
    "km8 = KMeans(n_clusters = 100)\n",
    "km8.fit(arr8)\n",
    "k8=km8.cluster_centers_\n",
    "\n",
    "km9 = KMeans(n_clusters = 100)\n",
    "km9.fit(arr9)\n",
    "k9=km9.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I get 1000 centroids of these clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 784)"
      ]
     },
     "execution_count": 718,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k9.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "Newdata = np.zeros((1000, 784))\n",
    "NewdataLabel = np.zeros(1000,dtype=int)\n",
    "NewdataLabel[:100] = 0\n",
    "NewdataLabel[100:200] = 1;NewdataLabel[200:300] = 2;NewdataLabel[300:400] = 3;NewdataLabel[400:500] = 4;\n",
    "NewdataLabel[500:600] = 5;NewdataLabel[600:700] = 6;NewdataLabel[700:800] = 7;NewdataLabel[800:900] = 8\n",
    "NewdataLabel[900:1000] = 9\n",
    "Newdata[:100, :] = k0\n",
    "Newdata[100:200, :] = k1\n",
    "Newdata[200:300, :] = k2\n",
    "Newdata[300:400, :] = k3\n",
    "Newdata[400:500, :] = k4\n",
    "Newdata[500:600, :] = k5\n",
    "Newdata[600:700, :] = k6\n",
    "Newdata[700:800, :] = k7\n",
    "Newdata[800:900, :] = k8\n",
    "Newdata[900:1000, :] = k9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will make the amount of training data smaller, since every cluster of the original training data will be replaced by a single centroid. There are 4000 original training data, and only 1000 new training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate the prediction accuracy and speed of k-NN using this new training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on new datasets with k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=10)"
      ]
     },
     "execution_count": 761,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose 5 neighbors\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(Newdata,NewdataLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True, False,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True, False,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True,  True, False,\n",
       "        True, False,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True, False,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True, False, False,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True, False, False,  True,  True,  True,  True,  True,  True,\n",
       "        True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True, False,  True,\n",
       "        True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "        True,  True,  True,  True, False, False,  True, False,  True,\n",
       "        True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True, False, False, False,  True,  True,  True, False,\n",
       "        True,  True,  True,  True, False,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True, False,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True,  True,  True,  True, False, False,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True, False, False,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True, False,\n",
       "        True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 762,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = knn.predict(Newdata)\n",
    "predictions == NewdataLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.908"
      ]
     },
     "execution_count": 763,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (predictions == NewdataLabel).sum()/len(NewdataLabel)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, some predicted labels do not match, but most of the time, k-NN can correctly predict each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n5 -r1\n",
    "x = range(10)\n",
    "for i in x:\n",
    "    knn = KNeighborsClassifier(n_neighbors=10)\n",
    "    knn.fit(Newdata,NewdataLabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using new training data, knn runs at a speed of 150ms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compared to the case of using unclustered training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=10)"
      ]
     },
     "execution_count": 765,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = knn.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9395"
      ]
     },
     "execution_count": 755,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (predictions == Y_train).sum()/len(Y_train)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of using unclustered training data is higher than that of using new training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.36 s ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n5 -r1\n",
    "x = range(10)\n",
    "for i in x:\n",
    "    knn = KNeighborsClassifier(n_neighbors=10)\n",
    "    knn.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using unclustered training dat, knn runs at a speed of 1.36s, which is slower than using new training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compared to the case of using the same amount of randomly selected training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the same amount of randomly selected training data, the size of X_train is 1000\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(images, labels, train_size=0.2, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=10)"
      ]
     },
     "execution_count": 770,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = knn.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.887"
      ]
     },
     "execution_count": 772,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (predictions == Y_train).sum()/len(Y_train)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of using the same amount of randomly selected training data is lower than that of using new training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n5 -r1\n",
    "x = range(10)\n",
    "for i in x:\n",
    "    knn = KNeighborsClassifier(n_neighbors=10)\n",
    "    knn.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the same amount of randomly selected training data, knn runs at a speed of 174ms, which is slower than using new training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
